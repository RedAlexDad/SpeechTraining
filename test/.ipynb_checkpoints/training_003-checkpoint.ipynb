{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc972fc-b185-41cd-a161-85f44d06b637",
   "metadata": {},
   "source": [
    "# Подключение библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d8703f-8b10-4734-9565-efb5876d2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d287cc5-2190-41ab-b9b1-8b7c0a1abda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import Resample\n",
    "from torch.nn import CTCLoss\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d5aff-f785-4c97-a5dc-d4287fe67eb5",
   "metadata": {},
   "source": [
    "## Загрузка датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f337058d-710e-40b1-8aad-7dcbac28981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка, существует ли уже сохраненный датасет\n",
    "if os.path.exists('/home/redalexdad/recognition_speech/common_voice_11/'):\n",
    "    # Загружаем сохраненный датасет, если он уже существует\n",
    "    cv_11_train = load_from_disk('/home/redalexdad/recognition_speech/common_voice_11/train/')\n",
    "    cv_11_test = load_from_disk('/home/redalexdad/recognition_speech/common_voice_11/test/')\n",
    "else:\n",
    "    # Иначе, скачиваем и сохраняем датасет\n",
    "    cv_11_train = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ru\", split=\"train\")\n",
    "    cv_11_train.save_to_disk('/home/redalexdad/recognition_speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264e6d-86ba-480b-838f-a19f113b64d6",
   "metadata": {},
   "source": [
    "## Содержимое датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ad6c72-7bdb-47f8-8d2d-97612dec5970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "    num_rows: 22862\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_11_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d7702c6-c484-4728-90a0-71f00ab9a0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "    num_rows: 9630\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_11_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a888fbea-6d9c-46c0-b8a5-89f2ae0b2dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '918b5e9edfb0aed8d7a73f938e07749e53fdda9babf808efe059e1ff3843b15b6e2e979fd619296e611965601e8219dc9f17de9dd480a08d8141942748e6f0ab',\n",
       " 'path': '/home/redalexdad/.cache/huggingface/datasets/downloads/extracted/d814cc3a56a5df3b5ccfa17b831afd6938306b9d17da77b602bb4b95387084b6/ru_train_0/common_voice_ru_26426765.mp3',\n",
       " 'audio': {'path': 'common_voice_ru_26426765.mp3',\n",
       "  'array': array([-1.06581410e-14,  8.34887715e-14,  8.08242362e-14, ...,\n",
       "         -2.88323849e-06,  1.16737738e-07,  9.74517661e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': 'Демократия неумолимо продвигается по Африке, и «арабская весна» была ее кульминацией.',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 1,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'ru',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_11_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dde6c-427a-4d19-91ff-2a5efcd0f0e7",
   "metadata": {},
   "source": [
    "## Создание класса датасета и обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01855c5d-e125-447d-8dca-569305ffd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_data = self.data[idx]['audio']['array']\n",
    "        target_text = self.data[idx]['sentence']\n",
    "        return audio_data, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90aed2d8-5e22-4d05-90a8-e5989061bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в тензоры и загрузчик данных\n",
    "def collate_fn(batch):\n",
    "    audios, texts = zip(*batch)\n",
    "    audio_lengths = [len(audio) for audio in audios]\n",
    "    texts = [text.lower() for text in texts]\n",
    "\n",
    "    # Применяем трансформации, такие как ресемплирование\n",
    "    # resample = Resample(orig_freq=48000, new_freq=16000)\n",
    "    # Уменьшаем частоту дискретизации\n",
    "    resample = Resample(orig_freq=48000, new_freq=8000)\n",
    "    audios = [resample(torch.FloatTensor(audio)) for audio in audios]\n",
    "\n",
    "    # Приводим все аудио к одной длине, например, путем дополнения нулями\n",
    "    audios = torch.nn.utils.rnn.pad_sequence(audios, batch_first=True)\n",
    "\n",
    "    # Добавляем третье измерение\n",
    "    audios = audios.unsqueeze(1)\n",
    "\n",
    "    # Преобразуем текст в числовое представление\n",
    "    char_map = {char: idx for idx, char in enumerate(set(''.join(texts)))}\n",
    "    target_lengths = [len(text) for text in texts]\n",
    "    targets = [torch.LongTensor([char_map[char] for char in text]) for text in texts]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    return audios, audio_lengths, targets, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ce60f-06f1-4a07-8678-bf7d66842f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Создаем char_map\n",
    "char_map = {char: idx for idx, char in enumerate(set(''.join([data_point['sentence'] for data_point in cv_11_train])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fc7a6-480b-4a28-96e5-e6d73a03f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация параметров\n",
    "input_size = 1  # Один канал для моноаудио\n",
    "hidden_size = 256  # Размер скрытого слоя\n",
    "num_classes = len(char_map)  # Количество классов, равное числу уникальных символов в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd2867-b23b-4a23-aeaa-c2a7ef9300b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Количество классов, равное числу уникальных символов в текстах: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acee84-44be-4c56-aa0d-1093f3aaaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0119a9-6730-4ca5-a07b-bfb9ce43c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель, оптимизатор и функцию потерь\n",
    "model = SpeechRecognitionModel(input_size, hidden_size, num_classes)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CTCLoss(blank=num_classes - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7456db-d841-407a-b44e-ea9e57f13bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводим модель и данные на GPU, если доступен\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec8e0b-cebc-4d3d-b81a-030228352728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Used device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2152b1-955b-40e5-8f6a-b755bd2a0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataLoader и обучаем модель\n",
    "dataset = SpeechDataset(cv_11_train)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa92e35f-d5fc-4b1f-b562-3d31b75dc34c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ваши параметры батча\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m audios, audio_lengths, targets, target_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Преобразование в тензоры\u001b[39;00m\n\u001b[1;32m      5\u001b[0m audio_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(audio_lengths)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Параметры батча\n",
    "audios, audio_lengths, targets, target_lengths = batch\n",
    "\n",
    "# Преобразование в тензоры\n",
    "audio_lengths = torch.LongTensor(audio_lengths)\n",
    "target_lengths = torch.LongTensor(target_lengths)\n",
    "\n",
    "# Проверка размерности\n",
    "assert audio_lengths.size(0) == audios.size(0), \"Размерность audio_lengths не соответствует размерности батча\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc78233a-2a18-4008-bd6e-c744f756fa90",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(audios)\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Обратное распространение и оптимизация\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_science/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_science/lib/python3.9/site-packages/torch/nn/modules/loss.py:1714\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_science/lib/python3.9/site-packages/torch/nn/functional.py:2460\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2455\u001b[0m         ctc_loss,\n\u001b[1;32m   2456\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2457\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2458\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[1;32m   2459\u001b[0m     )\n\u001b[0;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[1;32m   2462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "# Пример обучения на нескольких эпохах\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        audios, audio_lengths, targets, target_lengths = batch\n",
    "        audios, targets = audios.to(device), targets.to(device)\n",
    "        audios = audios.permute(0, 2, 1).to(device)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход и вычисление потерь\n",
    "        outputs = model(audios)\n",
    "        outputs = nn.functional.log_softmax(outputs, dim=2)\n",
    "        loss = criterion(outputs, targets, audio_lengths, target_lengths)\n",
    "\n",
    "        # Обратное распространение и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Обучение завершено.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab82bea-d54c-4c0e-b278-3ad34ded2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение обученной модели\n",
    "torch.save(model.state_dict(), 'speech_recognition_model.pth')\n",
    "print(\"Обучение завершено. Модель сохранена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37102eaa-4c8e-4a93-bff0-7a8ed081550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка на тестовом наборе данных\n",
    "model.eval()  # Установка модели в режим оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53831f03-3f44-4d31-acbc-6a9dc25b74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in test_dataloader:  # Предполагается, что у вас есть загрузчик данных для тестирования\n",
    "        audios, audio_lengths, targets, target_lengths = batch\n",
    "        audios, targets = audios.to(device), targets.to(device)\n",
    "        audios = audios.permute(0, 2, 1).to(device)\n",
    "\n",
    "        outputs = model(audios)\n",
    "        outputs = nn.functional.log_softmax(outputs, dim=2)\n",
    "        loss = criterion(outputs, targets, audio_lengths, target_lengths)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(2)\n",
    "        total += targets.size(1) * targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    average_test_loss = test_loss / len(test_dataloader)\n",
    "\n",
    "print(f\"Точность на тестовом наборе: {accuracy * 100:.2f}%\")\n",
    "print(f\"Средний loss на тестовом наборе: {average_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d3b2c-1d05-4c3f-8896-c58c999e03dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6104b07-83e0-4a29-9197-d6b00430b300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868c19a-aaad-4dd8-98b3-a8e0fa70446a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
